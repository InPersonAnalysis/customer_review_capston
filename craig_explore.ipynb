{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pip install kaggle\n",
    "2. Log-in to Kaggle (or sign up)\n",
    "3. Navigate to your Account page (click top-right profile picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/00/hwmfmnz96wxfytp1bbw2qlym0000gn/T/ipykernel_59061/2099535324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrangle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrangle_hotel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/codeup-data-science/customer_review_capstone/wrangle.py\u001b[0m in \u001b[0;36mwrangle_hotel\u001b[0;34m(use_cache)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Create columns for date types to groupby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/extension.py\u001b[0m in \u001b[0;36mfget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_date_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             result = self._maybe_mask_results(\n\u001b[1;32m    141\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float64\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = wrangle.wrangle_hotel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import unicodedata\n",
    "# import re\n",
    "\n",
    "# def basic_clean(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns the string normalized.\n",
    "#     '''\n",
    "#     string = unicodedata.normalize('NFKD', string)\\\n",
    "#              .encode('ascii', 'ignore')\\\n",
    "#              .decode('utf-8', 'ignore')\n",
    "#     string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "\n",
    "#     return string\n",
    "\n",
    "# def tokenize(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns a tokenized string.\n",
    "#     '''\n",
    "#     # Create tokenizer.\n",
    "#     tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "#     # Use tokenizer\n",
    "#     string = tokenizer.tokenize(string, return_str = True)\n",
    "\n",
    "#     return string\n",
    "\n",
    "# def stem(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns a string with words stemmed.\n",
    "#     '''\n",
    "#     # Create porter stemmer.\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "#     # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "#     stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "#     # Join our lists of words into a string again and assign to a variable.\n",
    "#     string = ' '.join(stems)\n",
    "    \n",
    "#     return string\n",
    "\n",
    "# def lemmatize(string):\n",
    "#     '''\n",
    "#     This function takes in string for and\n",
    "#     returns a string with words lemmatized.\n",
    "#     '''\n",
    "#     # Create the lemmatizer.\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "#     # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "#     lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "#     # Join our list of words into a string again and assign to a variable.\n",
    "#     string = ' '.join(lemmas)\n",
    "    \n",
    "#     return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "#     '''\n",
    "#     This function takes in a string, optional extra_words and exclude_words parameters\n",
    "#     with default empty lists and returns a string.\n",
    "#     '''\n",
    "#     # Create stopword_list.\n",
    "#     stopword_list = stopwords.words('english')\n",
    "    \n",
    "#     # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "#     stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "#     # Add in 'extra_words' to stopword_list.\n",
    "#     stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "#     # Split words in string.\n",
    "#     words = string.split()\n",
    "    \n",
    "#     # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "#     filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "#     # Join words in the list back into strings and assign to a variable.\n",
    "#     string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "#     return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.clean_pos_review.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nlp_clean(df):\n",
    "#     '''\n",
    "#     This function takes in a Customer Review Data and returns NLP prep.\n",
    "#     '''\n",
    "#     # Apply basic clean and tokenize to each review.\n",
    "#     df['positive_review'].apply(basic_clean).apply(tokenize)\n",
    "#     df['negative_review'].apply(basic_clean).apply(tokenize)\n",
    "#     # Apply stem to each review.\n",
    "#     df['positive_stem'] = [stem(review) for review in df.positive_review]\n",
    "#     df['negative_stem'] = [stem(review) for review in df.negative_review]\n",
    "#     # Apply lemmatize to each review.\n",
    "#     df['positive_lemma'] = [lemmatize(review) for review in df.positive_review]\n",
    "#     df['negative_lemma'] = [lemmatize(review) for review in df.negative_review]\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['positive_review'].apply(basic_clean).apply(tokenize)\n",
    "# df['positive_stem'] = [stem(review) for review in df.positive_review]\n",
    "# df['lemma_pos_review'] = [lemmatize(review) for review in df.positive_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_pos_review'] = df['positive_review'].apply(basic_clean)\n",
    "# df['clean_neg_review'] = df['negative_review'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['positive_token'] =tokenize(df['clean_pos_review'])\n",
    "# df['negative_token'] =tokenize(df['clean_neg_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['positve_stem'] = [stem(review) for review in df.clean_pos_review]\n",
    "# df['negative_stem'] = [stem(review) for review in df.clean_neg_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lemma_pos_review'] = [lemmatize(review) for review in df.clean_pos_review]\n",
    "# df['lemma_neg_review'] = [lemmatize(review) for review in df.clean_neg_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kaggle\n",
    "\n",
    "import kaggle\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "kaggle.api.dataset_download_files('jiashenliu/515k-hotel-reviews-data-in-europe', path='./' , unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pip install kaggle\n",
    "\n",
    "2. Log-in to Kaggle (or sign up)\n",
    "\n",
    "3. Navigate to your Account page (click top-right profile picture)\n",
    "\n",
    "4. API section on the Kaggle Account page.\n",
    "\n",
    "5. Scroll down to the API section and click Create New API Token\n",
    "\n",
    "6. Save kaggle.json to the file path displayed in the OSError message given when attempting to import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def acuire_hotel_data():\n",
    "#     kaggle.api.authenticate()\n",
    "\n",
    "#     kaggle.api.dataset_download_files('jiashenliu/515k-hotel-reviews-data-in-europe', path='./' , unzip=True)\n",
    "#      return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
